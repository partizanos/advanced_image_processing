{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tp3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/partizanos/advanced_image_processing/blob/master/tp3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lbwMHIDlsRC",
        "colab_type": "text"
      },
      "source": [
        "# Advanced Image Processing TP3\n",
        "\n",
        "Notes: The details of the analytical calculations must be included in the report. If you do not use an Equation Editor (for Word, Latex or similar) you can easily scan a handwritten page (readable!) and put it in the report.\n",
        "\n",
        "\n",
        "\n",
        "## Theoretical basis \n",
        "### Exercise 1. ...\n",
        "#### (a) Derive the entropy for the i.i.d. Laplacian and Gaussian distributions with the same variance.\n",
        "####  (b) Derive the Shannon lower bound for both distributions for `2-norm. Plot on the same figure. Make the conclusions which distribution is better compressible with the smaller distortion.\n",
        "#### (c) Derive the rate-distortion curve for the Laplacian distribution for `1-norm. Compare the obtained result with the above Shannon lower bound (obtained for `2-norm ). Plot both results on the same figure. Make the conclusions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkMlgB_EkmBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prerequistes\n",
        "## Setting configuration and installing requirements for the notebook\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>table { font-size: 2.5rem;width:100% !important; }</style>\"))\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wp4xw6YlH0H",
        "colab_type": "text"
      },
      "source": [
        "####  1.a Derive the entropy for the i.i.d. Laplacian and Gaussian distributions with the same variance.\n",
        "\n",
        "####  1.a.1  Derive the entropy for the i.i.d. Laplacian and Gaussian distributions with the same variance.\n",
        "\n",
        "|  Entropy | $ H(x) = - \\sum_{x} P(x) \\log_2 {P(x))} $ |\n",
        "|------    |                            ------|\n",
        "| Laplacian PDF | $ P(x) = \\frac{1}{2b}  e^{- \\frac{\\left\\vert x - \\mu  \\right\\vert } {b}} $\t\n",
        "|Laplacian entropy  | ${\\displaystyle \\log(2be)}$ |\n",
        "|Laplacian variance  | $ 2b^{2}$ |\n",
        "|Gasussian PDF | $ P(x) = \\frac{1}{ \\sqrt{ 2 \\pi \\sigma ^2}}  e^{ \\frac{( x - \\mu ) ^2} {2 \\sigma ^ 2}}  $\n",
        "|Gasussian Entropy | ${\\displaystyle {\\frac {1}{2}}\\log(2\\pi e\\sigma ^{2})}$ |\n",
        "|Gasussian variance | ${\\sigma ^{2}}$ |\n",
        "\n",
        "- When the variance is same among Gaussian and Laplacian probabillity distribution functions we have:\n",
        "<br>\n",
        "\n",
        "$ 2b^{2} = {\\sigma ^{2}} $ or <br>\n",
        "$ b^{2} = \\frac{\\sigma ^{2}}{2} $\n",
        "\n",
        "\n",
        "- Laplacian Entropy can be expressed as \n",
        "<br>\n",
        "\n",
        "$ H(x) = - \\sum_{x} P(x) \\log_2 {P(x))} $ \n",
        "<br>\n",
        "$H(x) = - \\sum_{x}$\n",
        "$\\frac{1}{2b}  e^{- \\frac{\\left\\vert x - \\mu  \\right\\vert } {b}}$\n",
        "$ \\log_2  $\n",
        "$(\\frac{1}{2b}  e^{- \\frac{\\left\\vert x - \\mu  \\right\\vert } {b}})$\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "$H(x) = - \\sum_{x}$\n",
        "$\\frac{1}{2b}$\n",
        "$ \\log_2  $\n",
        "$2^{  e^{- \\frac{\\left\\vert x - \\mu  \\right\\vert } {b}}}$\n",
        "$ \\log_2  $\n",
        "$(\\frac{1}{2b}  e^{- \\frac{\\left\\vert x - \\mu  \\right\\vert } {b}})$\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "${\\displaystyle \\log_{2}(2be)}= $\n",
        "${\\displaystyle \\log_{2}(2) + \\log_{2}(b) + \\log_{2}(e)} = $\n",
        "<br>\n",
        "Gaussian entropy (a) = ${\\displaystyle 1 + \\log_{2}(b) + \\log_{2}(e)}$\n",
        "\n",
        "\n",
        "####  1.a.2 Laplacian  Entropy can be expressed as \n",
        "\n",
        "$ {\\displaystyle {\\frac {1}{2}}\\log_{2}(2\\pi e\\sigma ^{2})} = $\n",
        "${\\displaystyle {\\frac {1}{2}}\\log_{2}(2\\pi e2b^{2} )}  = $\n",
        "${\\displaystyle {\\frac {1}{2}}\\log_{2}(4\\pi eb^{2} )}  = $\n",
        "<br>\n",
        "${\\displaystyle {\\frac {1}{2}}( \\log_{2}(4) + \\log_{2}(\\pi  ) + \\log_{2}(e) + \\log_{2}(b^{2}) })= $\n",
        "<br>\n",
        "${\\displaystyle {\\frac {1}{2}}( 1 + \\log_{2}(\\pi  ) + \\log_{2}(e) + 2\\log_{2}(b) }) = $\n",
        "<br>\n",
        "Gaussian entropy (b) = ${\\displaystyle \\frac {1}{2} + \\frac {\\log_{2}(\\pi) }{2} + \\frac{\\log_{2}(e)}{2} + \\log_{2}(b) }$\n",
        "\n",
        "\n",
        "- We notice by comparing (a) and (b)  that Laplacian entropy is larger \n",
        "<br>\n",
        "\n",
        "$${\\displaystyle 1 + \\log_{2}(b) + \\log_{2}(e)} > $$ \n",
        "$${\\displaystyle \\frac {1}{2} + \\frac {\\log_{2}(\\pi) }{2} + \\frac{\\log_{2}(e)}{2} + \\log_{2}(b) }$$\n",
        "<br>\n",
        "\n",
        "$${\\displaystyle \\frac {1}{2} + \\frac{\\log_{2}(e)}{2}}  > $$ \n",
        "$${\\displaystyle  \\frac {\\log_{2}(\\pi) }{2}   }$$\n",
        "<br>\n",
        "\n",
        "$${ \\displaystyle 1+  \\log_{2}(e)  > \\log_{2}(\\pi)    }$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$ {\\displaystyle 2e > \\pi  }  $$\n",
        "### The above result is also demonstrated with the following mini experiment"
      ]
    }
  ]
}